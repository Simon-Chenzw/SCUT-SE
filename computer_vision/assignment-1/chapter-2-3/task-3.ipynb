{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "178bcf80-463f-41d2-ba52-6e55b8ca9ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor, load_checkpoint, load_param_into_net\n",
    "\n",
    "import mindspore.dataset as ds\n",
    "from mindspore import context\n",
    "from mindspore.train import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "\n",
    "import os, pickle\n",
    "\n",
    "import mindspore.dataset.vision.c_transforms as CV\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "from mindspore.common import dtype as mstype\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37e8fe5f-ea5f-4231-a212-84e5d1beb62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''lenet'''\n",
    "\n",
    "class LeNet5(nn.Cell):\n",
    "    def __init__(self, num_class=10, channel=3):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channel, 6, 5, pad_mode=\"pad\")\n",
    "        self.deconv1 = nn.Conv2dTranspose(6, 3, 5, pad_mode=\"pad\")\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode=\"pad\")\n",
    "        self.fc1 = nn.Dense(16*5*5, 120)\n",
    "        self.fc2 = nn.Dense(120, 84)\n",
    "        self.fc3 = nn.Dense(84, num_class)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "    def construct(self, x):\n",
    "        #命名中间层输出\n",
    "        self.conv1_output = self.relu(self.conv1(x))\n",
    "        self.deconv1_output = self.deconv1(self.conv1_output)\n",
    "        self.pool1_output = self.pool(self.conv1_output)\n",
    "        self.conv2_output = self.relu(self.conv2(self.pool1_output))\n",
    "        self.pool2_output = self.pool(self.conv2_output)\n",
    "        x = self.flatten(self.pool2_output)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "network = LeNet5(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a739d49-922d-4598-9194-80c79f2753e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(datapath):\n",
    "    cifar_ds = ds.Cifar10Dataset(datapath)\n",
    "    return cifar_ds\n",
    "\n",
    "def process_dataset(cifar_ds,batch_size =32,status=\"train\"):\n",
    "    '''\n",
    "    ---- 定义算子 ----\n",
    "    '''\n",
    "    # 归一化\n",
    "    rescale = 1.0 / 255.0\n",
    "    # 平移\n",
    "    shift = 0.0\n",
    "\n",
    "    resize_op = CV.Resize((32, 32))\n",
    "    rescale_op = CV.Rescale(rescale, shift)\n",
    "    # 对于RGB三通道分别设定mean和std\n",
    "    normalize_op = CV.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    if status == \"train\":\n",
    "        # 随机裁剪\n",
    "        random_crop_op = CV.RandomCrop([32, 32], [4, 4, 4, 4])\n",
    "        # 随机翻转\n",
    "        random_horizontal_op = CV.RandomHorizontalFlip()\n",
    "    # 通道变化\n",
    "    channel_swap_op = CV.HWC2CHW()\n",
    "    # 类型变化\n",
    "    typecast_op = C.TypeCast(mstype.int32)\n",
    "\n",
    "    '''\n",
    "    ---- 算子运算 ----\n",
    "    '''\n",
    "    cifar_ds = cifar_ds.map(input_columns=\"label\", operations=typecast_op)\n",
    "    if status == \"train\":\n",
    "        cifar_ds = cifar_ds.map(input_columns=\"image\", operations=random_crop_op)\n",
    "        cifar_ds = cifar_ds.map(input_columns=\"image\", operations=random_horizontal_op)\n",
    "    cifar_ds = cifar_ds.map(input_columns=\"image\", operations=resize_op)\n",
    "    cifar_ds = cifar_ds.map(input_columns=\"image\", operations=rescale_op)\n",
    "    cifar_ds = cifar_ds.map(input_columns=\"image\", operations=normalize_op)\n",
    "    cifar_ds = cifar_ds.map(input_columns=\"image\", operations=channel_swap_op)\n",
    "    \n",
    "    # shuffle\n",
    "    cifar_ds = cifar_ds.shuffle(buffer_size=1000)\n",
    "    # 切分数据集到batch_size\n",
    "    cifar_ds = cifar_ds.batch(batch_size, drop_remainder=True)\n",
    "    \n",
    "    return cifar_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce300ef-aa68-4666-956e-563c6ca8ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回当前设备\n",
    "device_target = mindspore.context.get_context('device_target')\n",
    "# 确定图模型是否下沉到芯片上\n",
    "dataset_sink_mode = True if device_target in ['Ascend','GPU'] else False\n",
    "# 使用交叉熵函数作为损失函数\n",
    "net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n",
    "# 优化器为Adam\n",
    "net_opt = nn.Adam(params=network.trainable_params(), learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a2824a3-e7fe-4a72-b271-691711ee2ea2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint file does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10659/1784006953.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcurrent_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'results/checkpoint_lenet_original-100_1562.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/10-verify-bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindspore/train/serialization.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(ckpt_file_name, net, strict_load, filter_prefix)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mparam_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conv1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \"\"\"\n\u001b[0;32m--> 300\u001b[0;31m     \u001b[0mckpt_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_checkpoint_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Execute the process of loading checkpoint files.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0mcheckpoint_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MindSpore/lib/python3.7/site-packages/mindspore/train/serialization.py\u001b[0m in \u001b[0;36m_check_checkpoint_param\u001b[0;34m(ckpt_file_name, filter_prefix)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The checkpoint file does not exist.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mckpt_file_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\".ckpt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The checkpoint file does not exist."
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "checkpoint_path = os.path.join(current_path, 'results/checkpoint_lenet_original-100_1562.ckpt')\n",
    "load_checkpoint(checkpoint_path, net=network)\n",
    "\n",
    "data_path = os.path.join(current_path, 'data/10-verify-bin')\n",
    "batch_size=32\n",
    "status=\"test\"\n",
    "# 生成测试数据集\n",
    "cifar_ds = ds.Cifar10Dataset(data_path)\n",
    "ds_eval = process_dataset(cifar_ds,batch_size=batch_size,status=status)\n",
    "model = Model(network = network, loss_fn=net_loss,optimizer=net_opt, metrics={\"Accuracy\": Accuracy()})\n",
    "\n",
    "res = model.eval(ds_eval, dataset_sink_mode=dataset_sink_mode)\n",
    "# 评估测试集\n",
    "print('test results:',res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3a68e1-4fb2-45f6-b635-9cd67664e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale = 1.0 / 255.0\n",
    "shift = 0.0\n",
    "\n",
    "img = cv2.imread('0.jpg')\n",
    "resize = CV.Resize((32, 32))\n",
    "rescale = CV.Rescale(rescale, shift)\n",
    "normalize = CV.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "channel_op = CV.HWC2CHW()\n",
    "\n",
    "img = resize(img)\n",
    "img = rescale(img)\n",
    "img = normalize(img)\n",
    "img = channel_op(img)\n",
    "\n",
    "#添加一个维度batch_size\n",
    "img = [img]\n",
    "img = Tensor(img, mstype.float32)\n",
    "\n",
    "input = img\n",
    "output = network(input)\n",
    "\n",
    "print(network.deconv1_output.shape)\n",
    "layer_name = 'lenet5_deconv1_'\n",
    "image_name = '0'\n",
    "#conv1_output 1x6x28x28\n",
    "features = network.deconv1_output\n",
    "\n",
    "for i in range(features.shape[0]):\n",
    "    feature = features[i, :, :, :]\n",
    "    feature = np.transpose(feature.asnumpy(), [1, 2, 0])\n",
    "    #归一化\n",
    "    feature = (feature-np.amin(feature))/(np.amax(feature)-np.amin(feature))\n",
    "    plt.imshow(feature)\n",
    "plt.savefig('./featuremap/'+layer_name+image_name+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6e8704-5b2d-4a21-acea-ff560353ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['day_left', 'day_right', 'night_right']:\n",
    "    prepath = 'data/GardensPoint/'+dataset+'/'\n",
    "    images = [f for f in os.listdir(prepath) if f.endswith('.jpg')]\n",
    "    images.sort()\n",
    "\n",
    "    def preprocess(img):\n",
    "        rescale = 1.0 / 255.0\n",
    "        shift = 0.0\n",
    "        resize = CV.Resize((32, 32))\n",
    "        rescale = CV.Rescale(rescale, shift)\n",
    "        normalize = CV.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "        channel_op = CV.HWC2CHW()\n",
    "\n",
    "        img = resize(img)\n",
    "        img = rescale(img)\n",
    "        img = normalize(img)\n",
    "        img = channel_op(img)\n",
    "\n",
    "        #添加一个维度batch_size\n",
    "        #img = [img]\n",
    "        #img = Tensor(img, mstype.float32)\n",
    "        return img\n",
    "\n",
    "    input = []\n",
    "    for i in range(len(images)):\n",
    "        image = cv2.imread(prepath+images[i])\n",
    "        input.append(preprocess(image))\n",
    "    input = np.array(input)\n",
    "    input = Tensor(input, mstype.float32)\n",
    "\n",
    "    output = network(input)\n",
    "    with open('net_'+dataset+'_conv2', 'wb') as fw:\n",
    "        #conv1(200, 6, 28, 28),将tensor转化为numpy数组\n",
    "        pickle.dump(network.conv2_output.asnumpy(), fw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b19238-aa73-4e8f-9c76-08991d332607",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'conv2'\n",
    "with open('net_day_left_'+layer, 'rb') as fr:\n",
    "    Xdl = pickle.load(fr).reshape((200, -1))\n",
    "    \n",
    "with open('net_day_right_'+layer, 'rb') as fr:\n",
    "    Xdr = pickle.load(fr).reshape((200, -1))\n",
    "    \n",
    "with open('net_night_right_'+layer, 'rb') as fr:\n",
    "    Xnr = pickle.load(fr).reshape((200, -1))\n",
    "\n",
    "Xdl /= np.sqrt(np.sum(np.power(Xdl, 2), axis=1)).reshape(200, 1)\n",
    "Xdr /= np.sqrt(np.sum(np.power(Xdr, 2), axis=1)).reshape(200, 1)\n",
    "Xnr /= np.sqrt(np.sum(np.power(Xnr, 2), axis=1)).reshape(200, 1)\n",
    "\n",
    "#矩阵相乘\n",
    "D_nr_dr = Xnr.dot(Xdr.T)\n",
    "with open('D_nr_dr_'+layer, 'wb') as fw:\n",
    "    pickle.dump(D_nr_dr, fw)\n",
    "#plt.imshow(D_nr_dr)\n",
    "#plt.colorbar()\n",
    "#plt.title('night right vs day right')\n",
    "#plt.savefig('conv1_consine_nr_dr.png', bbox_inches='tight')\n",
    "#plt.show()\n",
    "\n",
    "D_dl_dr = Xdl.dot(Xdr.T)\n",
    "with open('D_dl_dr_'+layer, 'wb') as fw:\n",
    "    pickle.dump(D_dl_dr, fw)\n",
    "    \n",
    "D_nr_dl = Xnr.dot(Xdl.T)\n",
    "with open('D_nr_dl_'+layer, 'wb') as fw:\n",
    "    pickle.dump(D_nr_dl, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070a1cb-56c0-4193-acb9-ecf2d24184ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据集每一个场景都有对应的ground truth匹配，所以没有true negatives\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "#true-positive region: 相差正负e帧\n",
    "#论文中e取值1-5，根据数据集帧率\n",
    "e = 3\n",
    "#比率测试：最佳匹配与第二最佳匹配的余弦距离之比，如果大于r，则为positive\n",
    "r_range = np.arange(1, 2, .001)\n",
    "\n",
    "layers = ['conv2']\n",
    "for layer in layers:\n",
    "    \n",
    "    with open('D_nr_dr_'+layer, 'rb') as f:\n",
    "        D_nr_dr = pickle.load(f)\n",
    "\n",
    "    with open('D_dl_dr_'+layer, 'rb') as f:\n",
    "        D_dl_dr = pickle.load(f)\n",
    "    \n",
    "    with open('D_nr_dl_'+layer, 'rb') as f:\n",
    "        D_nr_dl = pickle.load(f)\n",
    "\n",
    "    #D_nr_dr：[200, 200]\n",
    "    n = D_nr_dr.shape[0]\n",
    "    AUC = []\n",
    "    \n",
    "    for D in [D_nr_dr, D_dl_dr, D_nr_dl]:\n",
    "        TP = np.zeros_like(r_range)\n",
    "        FP = np.zeros_like(r_range)\n",
    "        FN = np.zeros_like(r_range)\n",
    "        for i in range(len(r_range)):\n",
    "            r = r_range[i]\n",
    "            for row in range(n):\n",
    "                #numpy.argsort返回数组值由小到大的索引值\n",
    "                col2, col1 = np.argsort(D[row])[-2:]\n",
    "                if (D[row, col1] / D[row, col2]) >= r:\n",
    "                    \n",
    "                    if (row <= col1+e) and (row >= col1-e):\n",
    "                        TP[i] += 1\n",
    "                    else:\n",
    "                        FP[i] += 1\n",
    "                else:\n",
    "                    FN[i] += 1\n",
    "        #保留有效指标       \n",
    "        IX = (TP+FP) > 0\n",
    "        R = TP[IX] / (TP[IX] + FN[IX])\n",
    "        P = TP[IX] / (TP[IX] + FP[IX])\n",
    "        AUC.append(np.trapz(P[::-1], R[::-1]))\n",
    "        plt.plot(R, P)\n",
    "        # F1 = 2 * (P*R) / (P+R)\n",
    "        \n",
    "    labels = ['AUC=%.3f (night-right vs day-right)' %AUC[0],\n",
    "              'AUC=%.3f (day-left vs day-right)'    %AUC[1],\n",
    "              'AUC=%.3f (night-right vs day-left)'  %AUC[2]]\n",
    "    \n",
    "    p = np.arange(.001, 1, .001)\n",
    "    for f in np.arange(.1,1,.1):\n",
    "        r = np.array([f*v/(2*v-f) if 2*v!=f else -1 for v in p])\n",
    "        p_cut = p[np.logical_and(r>=0,r<=1)]\n",
    "        r_cut = r[np.logical_and(r>=0,r<=1)]\n",
    "        plt.plot(r_cut, p_cut, \"--\", color='gray')\n",
    "        plt.annotate(r\"$F_1=%.1f$\" % f, xy=(r_cut[0], p_cut[0]),\n",
    "            xytext=(.9, p_cut[0]), size=\"small\", color=\"gray\")\n",
    "\n",
    "    plt.legend(labels, loc='lower left')\n",
    "    plt.xlim([-0.01,1.01])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylim([-0.01,1.01])\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall curve for layer: {0}'.format(layer))\n",
    "    plt.savefig('PR_'+layer+'.png', bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
